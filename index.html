
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>AutoGUI: Scaling GUI Grounding with Autonomous Functionality Annotations from LLMs</title>

    <meta name="description" content="AutoGUI: Scaling GUI Grounding with Autonomous Functionality Annotations from LLMs">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <!-- <meta property="og:image" content="https://sheetcopilot.github.io/img/SheetCopilot-teaser.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1262">
    <meta property="og:image:height" content="694">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://sheetcopilot.github.io/"/>
    <meta property="og:title" content="SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models" />
    <meta property="og:description" content="Project page for SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models." /> -->

        <!--TWITTER-->
    <!-- <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models" />
    <meta name="twitter:description" content="Project page for SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models." />
    <meta name="twitter:image" content="https://palm-e.github.io/img/SheetCopilot-teaser.png" /> -->


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->
    <link rel="icon" type="image/x-icon" href="img/icon.png">

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script>

        timeoutIds = [];
        
        function populateDemo(imgs) {
            // Get the expanded image
            var expandImg = document.getElementById("expandedImg");
            // Get the image text
            var imgText = document.getElementById("imgtext");
            var answer = document.getElementById("answer");
        
            // Use the same src in the expanded image as the image being clicked on from the grid
            expandImg.src = imgs.src;
            // Use the value of the alt attribute of the clickable image as text inside the expanded image
            var qa = imgs.alt.split("[sep]");
            imgText.innerHTML = qa[0];
            answer.innerHTML = "";
            // Show the container element (hidden with CSS)
            expandImg.parentElement.style.display = "block";
            for (timeoutId of timeoutIds) {
                clearTimeout(timeoutId);
            }
            typeWriter(qa[1], 0, qa[0]);
            }
        
        function typeWriter(txt, i, q) {
            var imgText = document.getElementById("imgtext");
            if (imgText.innerHTML == q) {
            if (i < txt.length) {
                document.getElementById("answer").innerHTML += txt.charAt(i);
                i++;
                timeoutIds.push(setTimeout(typeWriter, 20, txt, i, q));
            }
            }
        }

        </script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-52J0PM8XKV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        
        <div class="row">
            <p></p> <p></p>
            <div class="col-md-12 text-center">
                <font size="+5"><b>AutoGUI: </b></font><br> <font size="+3">Scaling GUI Grounding with Autonomous Functionality Annotations from LLMs</font><br>
            </div>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <p>
                        Hongxin Li<sup>1,2*</sup>,
                        Jingran Sui<sup>3,4*</sup>, 
                        Jingran Su<sup>3,4*</sup>, 
                        Yuntao Chen<sup>3†</sup>, 
                        Qing Li<sup>4</sup>, 
                        Zhaoxiang Zhang<sup>1,2,3,5</sup>
                    </p>
                    <p>
                        <sup>1</sup>School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS) <br>
                        <sup>2</sup>State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences <br>
                        <sup>3</sup>Centre for Artificial Intelligence and Robotics (CAIR), Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences <br>
                        <sup>4</sup>The Hong Kong Polytechnic University     
                        <sup>5</sup>Shanghai Artificial Intelligence Laboratory
                    </p>
                    <p>
                        <sup>*</sup>Equal contribution.<br>
                        <sup>†</sup>Corresponding
                    </p>
                    
                    <br><br>
                    <!-- Institutes -->
                    <!-- <sup>1</sup><a href="http://g.co/robotics">
                        <img src="img/rng-logo.png" height="37px"> </a>
                        <sup>2</sup><a href="https://www.tu.berlin/en/">
                        <img src="img/tuberlin.png" height="32px"> </a> &nbsp;&nbsp;
                        <sup>3</sup><a href="https://research.google/teams/brain/">  
                        <img src="img/google-research-logo.png" height="25px"> </a>  -->
                        
                    </ul>

            </div>
        </div>


        <div class="row">
            <!-- <div class="col-md-8 col-md-offset-2">
                <p style="text-align:center;">
                    <img src="img/logo.png" class="img-responsive" width="45%">
                </p>
                <p class="text-center">
                    <b> SheetCopilot logo </b>
                </p>
                <br>
            </div> -->

            <div class="col-md-4 col-md-offset-4 text-center">
                
                
                <ul class="nav nav-pills nav-justified">
                    <!-- <li>
                        <a href="https://arxiv.org/abs/2305.19308">
                        <img src="img/paper_thumb.png" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li> -->

                    <!-- <li>
                        <a href="#demo">
                        <img src="img/SheetCopilot-teaser.png" height="60px">
                            <h4><strong>Demo</strong></h4>
                        </a>
                    </li>  -->
<!--                         <li>
                        <a href="http://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html">
                        <image src="img/google-ai-blog-small.png" height="60px">
                            <h4><strong>Blogpost</strong></h4>
                        </a>
                    </li> -->
                    <li>
                        <a href="https://github.com/BraveGroup">
                        <image src="img/github.png" height="60px">
                            <h4><strong>Training Data</strong></h4>
                        </a>                   
                    </li>
                    <li>
                        <a href="https://github.com/BraveGroup">
                        <image src="img/github.png" height="60px">
                            <h4><strong>Test Data (zipped)</strong></h4>
                        </a>                   
                    </li>
                     <li>
                        <a href="https://github.com/BraveGroup">
                        <image src="img/github.png" height="60px">
                            <h4><strong>Code</strong></h4>
                        </a>                   
                    </li>
                </ul>
            </div>
        </div>

        <!-- <section class="hero teaser">
            <div class="container is-max-widescreen">
              <div class="columns is-centered">
                <div class="column highlight is-8 content">
                  <span style="font-weight:bold">
                    AutoGUI provides a large-scale UI understanding dataset comprising 625k element functionality annotations.
                  </span>
                  <br>
                </div>
              </div>
          </div></section> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2 col-xs-12">
                <!-- <video id="v0" width="100%" playsinline muted loop autoplay onclick="setAttribute('controls', 'true');">
                    <source src="videos/SheetCopilot Demo2_1k.mp4" type="video/mp4">
                </video>		 -->

                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    User interface understanding with vision-language models has received much attention due to its potential for enabling next-generation software automation. However, existing UI datasets either only provide large-scale context-free element annotations or more contextualized functional descriptions for elements at a much smaller scale. In this work, we propose the AutoGUI pipeline for automatically annotating UI elements with detailed functionality descriptions at scale. Specifically, we leverage large language models (LLMs) to infer element functionality by comparing the UI source code changes before and after simulated interactions with specific UI elements. We construct an AutoGUI-627k dataset using the proposed pipeline, featuring multi-resolution, multi-device screenshots, diverse data domains, and detailed functionality annotations. Furthermore, we propose a spatially-aware instruction tuning approach for enhancing the grounding ability of UI vision-language models (UI-VLMs). Extensive experimental results demonstrate the superiority of our AutoGUI-627k dataset compared to existing web pre-training methods and the effectiveness of the proposed spatially-aware instruction tuning approach.
                </p>
            </div>
        </div>

        <div class="row" id="demo">
            <div class="col-md-8 col-md-offset-2 col-xs-12">
                <h3>
                    AutoGUI Data Samples
                </h3>
                
                
                <h3>
                    AutoGUI: A Scalable Pipeline that Provides Unlimited UI Element Functionality Data
                </h3>

            </div>
        </div>


<!-- 
        <div class="row" id="dataset">
            <div class="col-md-8 col-md-offset-2 col-xs-12">
                <h3>
                    Dataset
                </h3>
                <p class="text-justify">
                    We construct the SheetCopilot dataset, <a href="https://paperswithcode.com/dataset/sheetcopilot">a high-quality evaluation benchmark</a> as a foundation for assessing the spreadsheet control capabilities of various methods interested in this field, such as LLM-based methods and reinforcement learning-based methods.<br>
                    The dataset contains 28 evaluation workbooks and 221 spreadsheet manipulation tasks applied to these workbooks. These tasks involve diverse atomic actions related to the six task categories (i.e. Entry and manipulation, Formatting, Management, Charts, Pivot Table, and Formula).<br>
                    <b>Dataset statistics:</b><br>
                    1. Each task possesses one or more ground truth solutions.<br>
                    2. The lengths of the task instructions range from 20 to 530 characters, with most tasks between 80 and 110 characters.<br>
                    3. The numbers of atomic actions required by the tasks range from 1 to 9.<br>
                    <b>Evaluation metrics:</b>
                    <ul class="text-justify">
                        <li><b>Exec@1</b> measures the proportion of solutions executed without throwing exceptions.</li>
                        <li><b>Pass@1</b> is used to evaluate functional correctness.</li>
                        <li>Beyond correctness, we propose <b>A50</b> and <b>A90</b> scores to measure solution efficiency. These two metrics divide the number of atomic actions in a generated plan by the number in the ground truth and then calculate the 50th and 90th percentiles over all tasks. (Lower A50 and A90 scores indicate that the LLM tends to use fewer actions for completing a task.)</li>
                    </ul>
                    
                    A submitted solution is considered correct if the properties to be checked match those of any of the GT solutions of the corresponding task.<br>
                </p>
                <p><br></p>

                <!-- Dataset word clouds -->
                <p style="text-align:center;">
                    <img src="img/dataset_img/two_clouds.png" width="75%" alt="SheetCopilot example: Handling sales data.">
                </p>
                        
                <p class="text-justify">
                    <b>Dataset overview.</b> An overview of our dataset in shown by the wordclouds of the instructions and involved atomic actions. The two clouds show that our dataset contains diverse tasks that involve various spreadsheet operations.
                </p>
                <p><br></p>

                <!-- Cate proportions and donut chart -->
                <p style="text-align:center;">
                    <img src="img/dataset_img/CatePropAndVerbNoun.png" width="75%" alt="SheetCopilot example: Handling sales data.">
                </p>
                        
                <p class="text-justify">
                    ...
                </p>
                <p><br></p>

                <!-- Histograms -->
                <p style="text-align:center;">
                    <img src="img/dataset_img/Instruc&ActDistributions.png" width="75%" alt="SheetCopilot example: Handling sales data.">
                </p>
                        
                <p class="text-justify">
                    <b>Dataset statistics.</b>
                </p>
                <p><br></p>

                
            </div>
        </div>
        
    </div>

    <script>
    const myCarousel = document.getElementById('carouselExampleCaptions')
    myCarousel.addEventListener('slide.bs.carousel', event => {
        myCarousel.getElementsByClassName("carousel-item")[event.from].getElementsByTagName("video")[0].pause();
        myCarousel.getElementsByClassName("carousel-item")[event.to].getElementsByTagName("video")[0].play();
    })

    const myCarousel2 = document.getElementById('carouselExampleCaptions2')
    myCarousel2.addEventListener('slide.bs.carousel', event => {
        myCarousel2.getElementsByClassName("carousel-item")[event.from].getElementsByTagName("video")[0].pause();
        myCarousel2.getElementsByClassName("carousel-item")[event.to].getElementsByTagName("video")[0].play();
    })

    const myCarousel3 = document.getElementById('carouselExampleCaptions3')
    myCarousel3.addEventListener('slide.bs.carousel', event => {
        myCarousel3.getElementsByClassName("carousel-item")[event.from].getElementsByTagName("video")[0].pause();
        myCarousel3.getElementsByClassName("carousel-item")[event.to].getElementsByTagName("video")[0].play();
    })

    </script>
</body>
</html>
