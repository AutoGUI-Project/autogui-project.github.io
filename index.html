
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>AutoGUI: Scaling GUI Grounding with Autonomous Functionality Annotations from LLMs</title>

    <meta name="description" content="AutoGUI: Scaling GUI Grounding with Autonomous Functionality Annotations from LLMs">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <!-- <meta property="og:image" content="https://sheetcopilot.github.io/img/SheetCopilot-teaser.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1262">
    <meta property="og:image:height" content="694">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://sheetcopilot.github.io/"/>
    <meta property="og:title" content="SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models" />
    <meta property="og:description" content="Project page for SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models." /> -->

        <!--TWITTER-->
    <!-- <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models" />
    <meta name="twitter:description" content="Project page for SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models." />
    <meta name="twitter:image" content="https://palm-e.github.io/img/SheetCopilot-teaser.png" /> -->


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->
    <link rel="icon" type="image/x-icon" href="img/icon.png">

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script>

        timeoutIds = [];
        
        function populateDemo(imgs) {
            // Get the expanded image
            var expandImg = document.getElementById("expandedImg");
            // Get the image text
            var imgText = document.getElementById("imgtext");
            var answer = document.getElementById("answer");
        
            // Use the same src in the expanded image as the image being clicked on from the grid
            expandImg.src = imgs.src;
            // Use the value of the alt attribute of the clickable image as text inside the expanded image
            var qa = imgs.alt.split("[sep]");
            imgText.innerHTML = qa[0];
            answer.innerHTML = "";
            // Show the container element (hidden with CSS)
            expandImg.parentElement.style.display = "block";
            for (timeoutId of timeoutIds) {
                clearTimeout(timeoutId);
            }
            typeWriter(qa[1], 0, qa[0]);
            }
        
        function typeWriter(txt, i, q) {
            var imgText = document.getElementById("imgtext");
            if (imgText.innerHTML == q) {
            if (i < txt.length) {
                document.getElementById("answer").innerHTML += txt.charAt(i);
                i++;
                timeoutIds.push(setTimeout(typeWriter, 20, txt, i, q));
            }
            }
        }

        </script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-52J0PM8XKV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        <section class="header">
            <div class="row">
                <p></p> <p></p>
                <div class="col-md-12 text-center">
                    <font size="+5"><b><span class="gradient-color">AutoGUI</span>: </b></font><br> <font size="+3">Scaling GUI Grounding with Autonomous Functionality Annotations from LLMs</font><br>
                </div>
            </div>
            <div class="row">
                <div class="col-md-12 text-center">
                    <ul class="list-inline">
                        <p>
                            Hongxin Li<sup>1,2*</sup>,
                            Jingran Sui<sup>3,4*</sup>, 
                            Jingran Su<sup>3,4*</sup>, 
                            Yuntao Chen<sup>3†</sup>, 
                            Qing Li<sup>4</sup>, 
                            Zhaoxiang Zhang<sup>1,2,3,5</sup>
                        </p>
                        <p>
                            <sup>1</sup>School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS) <br>
                            <sup>2</sup>State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences <br>
                            <sup>3</sup>Centre for Artificial Intelligence and Robotics (CAIR), Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences <br>
                            <sup>4</sup>The Hong Kong Polytechnic University     
                            <sup>5</sup>Shanghai Artificial Intelligence Laboratory
                        </p>
                        <p>
                            <sup>*</sup>Equal contribution.<br>
                            <sup>†</sup>Corresponding
                        </p>
                        
                        <br><br>      
                        </ul>

                </div>
            </div>


        <div class="row">
            <!-- <div class="col-md-8 col-md-offset-2">
                <p style="text-align:center;">
                    <img src="img/logo.png" class="img-responsive" width="45%">
                </p>
                <p class="text-center">
                    <b> SheetCopilot logo </b>
                </p>
                <br>
            </div> -->

            <div class="col-md-4 col-md-offset-4 text-center">
                
                
                <ul class="nav nav-pills nav-justified">
                    <!-- <li>
                        <a href="https://arxiv.org/abs/2305.19308">
                        <img src="img/paper_thumb.png" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li> -->

                    <!-- <li>
                        <a href="#demo">
                        <img src="img/SheetCopilot-teaser.png" height="60px">
                            <h4><strong>Demo</strong></h4>
                        </a>
                    </li>  -->
<!--                         <li>
                        <a href="http://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html">
                        <image src="img/google-ai-blog-small.png" height="60px">
                            <h4><strong>Blogpost</strong></h4>
                        </a>
                    </li> -->
                    <li>
                        <a href="https://github.com/BraveGroup">
                        <image src="img/data_icon.png" height="60px">
                            <h4><strong>Training Data</strong></h4>
                        </a>                   
                    </li>
                    <li>
                        <a href="https://github.com/BraveGroup">
                        <image src="img/zip_icon.png" height="60px">
                            <h4><strong>Test Data (zipped)</strong></h4>
                        </a>                   
                    </li>
                     <li>
                        <a href="https://github.com/BraveGroup">
                        <image src="img/github.png" height="60px">
                            <h4><strong>Code</strong></h4>
                        </a>                   
                    </li>
                </ul>
            </div>
        </div>
    </section>
        <!-- <section class="hero teaser">
            <div class="container is-max-widescreen">
              <div class="columns is-centered">
                <div class="column highlight is-8 content">
                  <span style="font-weight:bold">
                    AutoGUI provides a large-scale UI understanding dataset comprising 625k element functionality annotations.
                  </span>
                  <br>
                </div>
              </div>
          </div></section> -->
    
    <section>
        <div class="columns is-centered">
            <h3><span class="gradient-color" style="font-weight:bold">AutoGUI</span> provides high-quality functionality annotations for User Interface (UI) elements in a scalable&#128640 manner, establishing itself as a cornerstone for building intelligent UI agents.</h3>
            
            <div class="column highlight is-8 content">
              
                
                <br>
              <video class="simple-demo" controls="" autoplay="" playsinline="" muted="" loop="" poster="demo/thumbnail.png">
                <source src="demo/func_anno_simple_demo.mp4" type="video/mp4">
              </video>
              <br>
            </div>
          </div>
    </section>

    <section  class="col-md-8 col-md-offset-2 col-xs-12">
        <div class="row">
            <div>
                <!-- <video id="v0" width="100%" playsinline muted loop autoplay onclick="setAttribute('controls', 'true');">
                    <source src="videos/SheetCopilot Demo2_1k.mp4" type="video/mp4">
                </video>		 -->

                <h3 class="title is-3">
                    Abstract
                </h3>
                <p class="text-justify">
                    User interface understanding with vision-language models has received much attention due to its potential for enabling next-generation software automation. However, existing UI datasets either only provide large-scale context-free element annotations or more contextualized functional descriptions for elements at a much smaller scale. In this work, we propose the AutoGUI pipeline for automatically annotating UI elements with detailed functionality descriptions at scale. Specifically, we leverage large language models (LLMs) to infer element functionality by comparing the UI source code changes before and after simulated interactions with specific UI elements. We construct an AutoGUI-627k dataset using the proposed pipeline, featuring multi-resolution, multi-device screenshots, diverse data domains, and detailed functionality annotations. Furthermore, we propose a spatially-aware instruction tuning approach for enhancing the grounding ability of UI vision-language models (UI-VLMs). Extensive experimental results demonstrate the superiority of our AutoGUI-627k dataset compared to existing web pre-training methods and the effectiveness of the proposed spatially-aware instruction tuning approach.
                </p>
            </div>
        </div>

        <div class="">
            <md-block rendered="content"><p><em>Updates</em></p>
        <ul>
        <li>2024-6-14: AutoGUI-v1 released.</li>
        </ul>
        </md-block>
                </div>
    </section>

    <br><br>
    <section  class="col-md-8 col-md-offset-2 col-xs-12">
        <div class="row" id="demo">
            <div>
                <h3>
                    Overview of AutoGUI Dataset
                </h3>
                
                <p>
                    <span class="gradient-color">AutoGUI</span> contains 627k UI grounding/captioning tasks involving 861 domains from Common Crawl. Different from exsiting UI datasets, AutoGUI provides abundant UI functionality annotations and focuses on functionality grounding/captioning tasks. Moreover, these tasks are all automatically generated without human experts. 
                </p><br>

                <p>Each sample in the training set serves as either a grounding or captioning task, containing the following information:</p>
                <ul>
                    <li> <code>Functionality</code> that describes the contextual functionality of an element.</li>
                    <li> <code>Target Element</code> that provides the attributes of the element associated with the functionality label.</li>
                    <ul>
                      <li>Element Position: the original bounding box coordinates <code>[left,top,right,bottom]</code> as well as the center point <code>(X,Y)</code>normalized within the range 0-100 are provided.
                      </li>
                      <li>Element Text: The displayed text of alt-text of the element.
                      </li>
                      <li>Device Type: Whether the task is collected in a view of a web browser or mobile phone.
                    </li>
                    </ul>
                    <li><code>UI Screenshot</code> that records the UI image on which the element is displayed:
                  </li></ul>
            </div>
        </div>
        
        <br>
        <h3>
            Dataset Comparison
        </h3>
        <p><span class="gradient-color" style="font-weight:bold">AutoGUI</span> distinguishes itself from existing dataset by providing functionality-rich data as well as tasks that require VLMs to discern the functionalities of substantial elements to achieve high grounding accuracy.</p>
        <br>
        <div class="scrollable-table">
            <table>
                <thead>
                    <tr>
                        <th>Dataset</th>
                        <th>UI Type</th>
                        <th>Multi-Res</th>
                        <th style="width: 15%">Real-world<br>Scenario</th>
                        <th>Autonomous Annotation</th>
                        <th>Functionality Sementics</th>
                        <th>#Annotations</th>
                        <th>UI Task</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><a href="https://webshop-pnlp.github.io/">WebShop</a></td><td>Web</td><td>✖</td><td>✖</td><td>✖</td><td>✖</td><td>12k</td><td>Web Navigation</td>
                    </tr>
                    <tr>
                        <td><a href="https://osu-nlp-group.github.io/Mind2Web/">Mind2Web</a></td><td>Web</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>2.4k</td><td>Web Navigation</td>
                    </tr>
                    <tr>
                        <td><a href="https://webarena.dev/">WebArena</a></td><td>Web</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>812</td><td>Web Navigation</td>
                    </tr>
                    <tr>
                        <td><a href="https://paperswithcode.com/dataset/screen2words">Screen2Words</a></td><td>Mobile</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>112k</td><td>UI Summarization</td>
                    </tr>
                    <tr>
                        <td><a href="https://github.com/google-research-datasets/widget-caption">Widget Captioning</a></td><td>Mobile</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>163k</td><td>Element Captioning</td>
                    </tr>
                    <tr>
                        <td><a href="https://paperswithcode.com/dataset/pixelhelp">PixelHelp</a></td><td>Mobile</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>187</td><td>Element Grounding</td>
                    </tr>
                    <tr>
                        <td><a href="https://paperswithcode.com/dataset/ricosca"></a>RICOSCA</td><td>Mobile</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>295k</td><td>Action Grounding</td>
                    </tr>
                    <tr>
                        <td><a href="https://github.com/aburns4/MoTIF"></a>MoTIF</td><td>Mobile</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>6k</td><td>Mobile Navigation</td>
                    </tr>
                    <tr>
                        <td><a href="https://github.com/google-research/google-research/tree/master/android_in_the_wild">AITW</a></td><td>Mobile</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>715k</td><td>Mobile Navigation</td>
                    </tr>
                    <tr>
                        <td><a href="https://github.com/google-research-datasets/uibert">RefExp</a></td><td>Mobile</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>20.8k</td><td>Element Grounding</td>
                    </tr>
                    <tr>
                        <td><a href="https://visualwebbench.github.io/">VisualWebBench</a></td><td>Web</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>1.5k</td><td>UI Grounding & Referring</td>
                    </tr>
                    <tr>
                        <td><a href="https://github.com/njucckevin/SeeClick">SeeClick Web</a></td><td>Web</td><td>✖</td><td>✔</td><td>✔</td><td>✖</td><td>271k</td><td>Element Grounding</td>
                    </tr>
                    <tr>
                        <td><a href="https://github.com/THUDM/CogAgent">UI REC/REG</a></td><td>Web</td><td>✔</td><td>✔</td><td>✔</td><td>✖</td><td>400k</td><td>Box2DOM, DOM2Box</td>
                    </tr>
                    <tr>
                        <td><a href="https://arxiv.org/abs/2404.05719">Ferret-UI</a></td><td>Mobile</td><td>✔</td><td>✔</td><td>✔</td><td>✖</td><td>250k</td><td>UI Grounding & Referring</td>
                    </tr>
                    <tr>
                        <td><span class="gradient-color" style="font-weight:bold">AutoGUI</span> (ours)</td><td>Web, Mobile</td><td>✔</td><td>✔</td><td>✔</td><td>✔</td><td>627k</td><td>Functionality Grounding & Captioning</td>
                    </tr>
                    <!-- More rows here -->
                </tbody>

            </table>
        </div>
        
        <br><br>
        <h3>Explore the <span class="gradient-color">AutoGUI</span> Dataset</h3>
        <p>The following explorer allows you to look into several examples of the training data. You can click the dashed blue box to view its functionality annotation generate by our AutoGUI pipeline.
        </p>

        <div class="slider">
            <div class="content" id="content">
                <div class="image-container">
                    <img id="autogui-image" src="" alt="Random Image">
                </div>
            </div>
            <div class="sidebar">
                <ul id="funcs"></ul>
            </div>
            <div class="buttons">
                <button class="nav-btn" onclick="showPreviousContent()">Previous</button>
                <button class="nav-btn" onclick="showNextContent()">Next</button>
                <button class="nav-btn" onclick="showRandomContent()">Random</button>
            </div>
        </div>

    </section>

    <section  class="col-md-8 col-md-offset-2 col-xs-12">
        <h3><span class="gradient-color">AutoGUI</span> Characteristics</h3>
        <p>Diversity of the verb-noun phrases of the AutoGUI dataset. The top 10 verbs and their top 5 following nouns are displayed to show that our dataset involve diverse UI functionalities.</p>

        <iframe src="verbnoun_sunburst.html" width="100%" height="600" style="border:none;"></iframe>

        <br><br>
        <p>The word cloud below illustrates the relative frequencies of the verbs that represent the primary intended actions described in the functionality annotations.</p>
        <p style="text-align:center;">
            <img src="img/word_cloud.png" width="75%" alt="AutoGUI word cloud">
        </p>
    </section>

    <section  class="col-md-8 col-md-offset-2 col-xs-12">
        <h3>Disclaimer</h3>
        <p>This dataset has been compiled and disseminated exclusively for research purposes, with the objective of enhancing web accessibility through the application of language technologies. Any comercial use of the data is not permitted.</p>

    </section>

                
            </div>
        </div>
        
    </div>

</body>
</html>
