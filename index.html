
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>AutoGUI: Scaling GUI Grounding with Autonomous Functionality Annotations from LLMs</title>

    <meta name="description" content="AutoGUI: Scaling GUI Grounding with Autonomous Functionality Annotations from LLMs">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <!-- <meta property="og:image" content="https://sheetcopilot.github.io/img/SheetCopilot-teaser.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1262">
    <meta property="og:image:height" content="694">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://sheetcopilot.github.io/"/>
    <meta property="og:title" content="SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models" />
    <meta property="og:description" content="Project page for SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models." /> -->

        <!--TWITTER-->
    <!-- <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models" />
    <meta name="twitter:description" content="Project page for SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models." />
    <meta name="twitter:image" content="https://palm-e.github.io/img/SheetCopilot-teaser.png" /> -->


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->
    <link rel="icon" type="image/x-icon" href="img/icon.png">

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script>

        timeoutIds = [];
        
        function populateDemo(imgs) {
            // Get the expanded image
            var expandImg = document.getElementById("expandedImg");
            // Get the image text
            var imgText = document.getElementById("imgtext");
            var answer = document.getElementById("answer");
        
            // Use the same src in the expanded image as the image being clicked on from the grid
            expandImg.src = imgs.src;
            // Use the value of the alt attribute of the clickable image as text inside the expanded image
            var qa = imgs.alt.split("[sep]");
            imgText.innerHTML = qa[0];
            answer.innerHTML = "";
            // Show the container element (hidden with CSS)
            expandImg.parentElement.style.display = "block";
            for (timeoutId of timeoutIds) {
                clearTimeout(timeoutId);
            }
            typeWriter(qa[1], 0, qa[0]);
            }
        
        function typeWriter(txt, i, q) {
            var imgText = document.getElementById("imgtext");
            if (imgText.innerHTML == q) {
            if (i < txt.length) {
                document.getElementById("answer").innerHTML += txt.charAt(i);
                i++;
                timeoutIds.push(setTimeout(typeWriter, 20, txt, i, q));
            }
            }
        }

        </script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-52J0PM8XKV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        <section class="header">
            <div class="row">
                <p></p> <p></p>
                <div class="col-md-12 text-center">
                    <font size="+5"><b><span class="gradient-color">AutoGUI</span>: </b></font><br> <font size="+3">Scaling GUI Grounding with Autonomous Functionality Annotations from LLMs</font><br>
                </div>
            </div>
            <div class="row">
                <div class="col-md-12 text-center">
                    <ul class="list-inline">
                        <p>
                            Hongxin Li<sup>*1,2</sup>,
                            Jingran Sui<sup>*3,4</sup>, 
                            Jingran Su<sup>*3,4</sup>, 
                            Yuntao Chen<sup>†3</sup>, 
                            Qing Li<sup>†4</sup>, 
                            Zhaoxiang Zhang<sup>†1,2,3,5</sup>
                        </p>
                        <p>
                            <sup>1</sup>School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS) <br>
                            <sup>2</sup>State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences <br>
                            <sup>3</sup>Centre for Artificial Intelligence and Robotics (CAIR), Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences <br>
                            <sup>4</sup>The Hong Kong Polytechnic University     
                            <sup>5</sup>Shanghai Artificial Intelligence Laboratory
                        </p>
                        <p>
                            <sup>*</sup>Equal contribution.<br>
                            <sup>†</sup>Corresponding
                        </p>
                        
                        <br><br>      
                        </ul>

                </div>
            </div>


        <div class="row">
            <!-- <div class="col-md-8 col-md-offset-2">
                <p style="text-align:center;">
                    <img src="img/logo.png" class="img-responsive" width="45%">
                </p>
                <p class="text-center">
                    <b> SheetCopilot logo </b>
                </p>
                <br>
            </div> -->

            <div class="col-md-4 col-md-offset-4 text-center">
                
                
                <ul class="nav nav-pills nav-justified">
                    <!-- <li>
                        <a href="https://arxiv.org/abs/2305.19308">
                        <img src="img/paper_thumb.png" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li> -->

                    <!-- <li>
                        <a href="#demo">
                        <img src="img/SheetCopilot-teaser.png" height="60px">
                            <h4><strong>Demo</strong></h4>
                        </a>
                    </li>  -->
<!--                         <li>
                        <a href="http://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html">
                        <image src="img/google-ai-blog-small.png" height="60px">
                            <h4><strong>Blogpost</strong></h4>
                        </a>
                    </li> -->
                    <li>
                        <a href="https://huggingface.co/datasets/AutoGUI/AutoGUI-v1-702k">
                        <image src="img/data_icon.png" height="60px">
                            <h4><strong>Training Data</strong></h4>
                        </a>                   
                    </li>
                    <li>
                        <a href="https://drive.google.com/file/d/1VfhVtcN1B4WSrb4DXJczQQ5zx1_pOvBu/view?usp=sharing">
                        <image src="img/zip_icon.png" height="60px">
                            <h4><strong>Test Data (zipped)</strong></h4>
                        </a>                   
                    </li>
                     <li>
                        <a href="https://github.com/ZJULiHongxin/AutoGUI">
                        <image src="img/github.png" height="60px">
                            <h4><strong>Code</strong></h4>
                        </a>                   
                    </li>
                    <li>
                        <a href="https://huggingface.co/HongxinLi/AutoGUI-Qwen-v0.1">
                        <image src="img/model.jpg" height="60px">
                            <h4><strong>Model</strong></h4>
                        </a>                   
                    </li>
                </ul>
            </div>
        </div>
    </section>
        <!-- <section class="hero teaser">
            <div class="container is-max-widescreen">
              <div class="columns is-centered">
                <div class="column highlight is-8 content">
                  <span style="font-weight:bold">
                    AutoGUI provides a large-scale UI understanding dataset comprising 625k element functionality annotations.
                  </span>
                  <br>
                </div>
              </div>
          </div></section> -->
    
    <section>
        <div class="columns is-centered">
            <h3><span class="gradient-color" style="font-weight:bold">AutoGUI</span> provides high-quality functionality annotations for User Interface (UI) elements in a scalable&#128640 manner, establishing itself as a cornerstone for building intelligent UI agents.</h3>
            
            <div class="column highlight is-8 content">
              
                
                <br>
              <video class="simple-demo" controls="" autoplay="" playsinline="" muted="" loop="" poster="demo/thumbnail.png">
                <source src="demo/func_anno_simple_demo.mp4" type="video/mp4">
              </video>
              <br>
            </div>
          </div>
    </section>

    <section  class="col-md-8 col-md-offset-2 col-xs-12">
        <div class="row">
            <div>
                <!-- <video id="v0" width="100%" playsinline muted loop autoplay onclick="setAttribute('controls', 'true');">
                    <source src="videos/SheetCopilot Demo2_1k.mp4" type="video/mp4">
                </video>		 -->

                <h3 class="title is-3">
                    Abstract
                </h3>
                <p class="text-justify">
                    User interface understanding with vision-language models has received much attention due to its potential for enabling next-generation software automation. However, existing UI datasets either only provide large-scale context-free element annotations or more contextualized functional descriptions for elements at a much smaller scale. In this work, we propose the AutoGUI pipeline for automatically annotating UI elements with detailed functionality descriptions at scale. Specifically, we leverage large language models (LLMs) to infer element functionality by comparing the UI source code changes before and after simulated interactions with specific UI elements. We construct an AutoGUI-627k dataset using the proposed pipeline, featuring multi-resolution, multi-device screenshots, diverse data domains, and detailed functionality annotations. Furthermore, we propose a spatially-aware instruction tuning approach for enhancing the grounding ability of UI vision-language models (UI-VLMs). Extensive experimental results demonstrate the superiority of our AutoGUI-627k dataset compared to existing web pre-training methods and the effectiveness of the proposed spatially-aware instruction tuning approach.
                </p>
            </div>
        </div>

        <div class="">
            <md-block rendered="content"><p><em>Updates</em> <strong><a href="https://zhaoxiangzhang.net/wp-content/uploads/2020/02/new.gif"><img decoding="async" class="alignnone size-full wp-image-105" src="https://zhaoxiangzhang.net/wp-content/uploads/2020/02/new.gif" alt="new" width="29" height="13"></a></strong></p>
        <ul>
        <li> ✨ 2024-06-14: AutoGUI-v1 was released.</li>
        </ul>
        </md-block>
                </div>
    </section>

    <br><br>
    <section  class="col-md-8 col-md-offset-2 col-xs-12">
        <div class="row" id="demo">
            <div>
                <h3>
                    Overview of AutoGUI Dataset
                </h3>
                
                <p>
                    <span class="gradient-color">AutoGUI</span> contains 627k UI grounding/captioning tasks involving 861 domains from Common Crawl. Different from exsiting UI datasets, AutoGUI provides abundant UI functionality annotations without human experts and focuses on functionality grounding/captioning tasks.
                    
                    <br><br>
                    <b>Task 1: Functionality Grounding</b>: Given a user's instruction that describes UI functionalities, models are prompted to output the 2D coordinates of the element targeted by the instruction. Note that the coordinates are normalized withing [0,100)
                    <br>
                    Example:
                    <br>
                    
                    <div class="task-demo-section">
                        <img src="img/func_grounding_sample.jpg" alt="Example Image" class="task-image">
                        <div class="conversation">
                            <p><b>User</b>: In this web page image, please locate the element as I describe it (with point). This element triggers a user registration process, allowing new users to create a PayPal account and gain access to the platform's services.</p>
                            <p><b>Assistant</b>: (91,6)</p>
                        </div>
                    </div>

                    <b>Task 2: Functionality Captioning</b>: Given a user's instruction that refers to a certain location in the format of a point, models are prompted to describe the functionality of the pointed element considering the UI context.
                    <br>
                    Example:
                    <br>
                    
                    <div class="cap-task-demo-section">
                        <img src="img/func_captioning_sample.jpg" alt="Example Image" class="cap-task-image">
                        <div class="cap-task-conversation">
                            <p><b>User</b>: What happens when you tap position (61,73) on the screen?</p>
                            <p><b>Assistant</b>: This element serves as an input field for users to provide their birth date, contributing to the registration process by ensuring that users meet the age requirements for creating a Yahoo account.</p>
                        </div>
                    </div>

                </p><br>

                <p>Every sample in the training set contains the following information:</p>
                <ul>
                    <li> <code>Functionality</code> that describes the contextual functionality of an element.</li>
                    <li> <code>Target Element</code> that provides the attributes of the element associated with the functionality label.</li>
                    <ul>
                      <li>Element Position: the original bounding box coordinates <code>[left,top,right,bottom]</code> as well as the center point <code>(X,Y)</code>normalized within the range 0-100 are provided.
                      </li>
                      <li>Element Text: The displayed text of alt-text of the element.
                      </li>
                      <li>Device Type: Whether the task is collected in a view of a web browser or mobile phone.
                    </li>
                    </ul>
                    <li><code>UI Screenshot</code> that records the UI image on which the element is displayed:
                  </li></ul>
            </div>
        </div>
        
        <br>
        <h3>
            Dataset Comparison
        </h3>
        <p><span class="gradient-color" style="font-weight:bold">AutoGUI</span> distinguishes itself from existing dataset by providing functionality-rich data as well as tasks that require VLMs to discern the functionalities of substantial elements to achieve high grounding accuracy.</p>
        <br>
        <div class="scrollable-table">
            <table>
                <thead>
                    <tr>
                        <th>Dataset</th>
                        <th>UI Type</th>
                        <th>Multi-Res</th>
                        <th style="width: 15%">Real-world<br>Scenario</th>
                        <th>Autonomous Annotation</th>
                        <th>Functionality Sementics</th>
                        <th>#Annotations</th>
                        <th>UI Task</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><a href="https://webshop-pnlp.github.io/">WebShop</a></td><td>Web</td><td>✖</td><td>✖</td><td>✖</td><td>✖</td><td>12k</td><td>Web Navigation</td>
                    </tr>
                    <tr>
                        <td><a href="https://osu-nlp-group.github.io/Mind2Web/">Mind2Web</a></td><td>Web</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>2.4k</td><td>Web Navigation</td>
                    </tr>
                    <tr>
                        <td><a href="https://webarena.dev/">WebArena</a></td><td>Web</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>812</td><td>Web Navigation</td>
                    </tr>
                    <tr>
                        <td><a href="https://paperswithcode.com/dataset/screen2words">Screen2Words</a></td><td>Mobile</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>112k</td><td>UI Summarization</td>
                    </tr>
                    <tr>
                        <td><a href="https://github.com/google-research-datasets/widget-caption">Widget Captioning</a></td><td>Mobile</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>163k</td><td>Element Captioning</td>
                    </tr>
                    <tr>
                        <td><a href="https://paperswithcode.com/dataset/pixelhelp">PixelHelp</a></td><td>Mobile</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>187</td><td>Element Grounding</td>
                    </tr>
                    <tr>
                        <td><a href="https://paperswithcode.com/dataset/ricosca">RICOSCA</a></td><td>Mobile</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>295k</td><td>Action Grounding</td>
                    </tr>
                    <tr>
                        <td><a href="https://github.com/aburns4/MoTIF">MoTIF</a></td><td>Mobile</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>6k</td><td>Mobile Navigation</td>
                    </tr>
                    <tr>
                        <td><a href="https://github.com/google-research/google-research/tree/master/android_in_the_wild">AITW</a></td><td>Mobile</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>715k</td><td>Mobile Navigation</td>
                    </tr>
                    <tr>
                        <td><a href="https://github.com/google-research-datasets/uibert">RefExp</a></td><td>Mobile</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>20.8k</td><td>Element Grounding</td>
                    </tr>
                    <tr>
                        <td><a href="https://visualwebbench.github.io/">VisualWebBench</a></td><td>Web</td><td>✖</td><td>✔</td><td>✖</td><td>✖</td><td>1.5k</td><td>UI Grounding & Referring</td>
                    </tr>
                    <tr>
                        <td><a href="https://github.com/njucckevin/SeeClick">SeeClick Web</a></td><td>Web</td><td>✖</td><td>✔</td><td>✔</td><td>✖</td><td>271k</td><td>Element Grounding</td>
                    </tr>
                    <tr>
                        <td><a href="https://github.com/THUDM/CogAgent">UI REC/REG</a></td><td>Web</td><td>✔</td><td>✔</td><td>✔</td><td>✖</td><td>400k</td><td>Box2DOM, DOM2Box</td>
                    </tr>
                    <tr>
                        <td><a href="https://arxiv.org/abs/2404.05719">Ferret-UI</a></td><td>Mobile</td><td>✔</td><td>✔</td><td>✔</td><td>✖</td><td>250k</td><td>UI Grounding & Referring</td>
                    </tr>
                    <tr>
                        <td><span class="gradient-color" style="font-weight:bold">AutoGUI</span> (ours)</td><td>Web, Mobile</td><td>✔</td><td>✔</td><td>✔</td><td>✔</td><td>627k</td><td>Functionality Grounding & Captioning</td>
                    </tr>
                    <!-- More rows here -->
                </tbody>

            </table>
        </div>
        
        <br><br>
        <h3>Explore the <span class="gradient-color">AutoGUI</span> Dataset</h3>
        <p>The following explorer allows you to look into several examples of the training data. You can click one of the dashed blue boxes to view its functionality annotation automatically generated by our AutoGUI pipeline.
        </p>

        <div class="slider">
            <div class="content" id="content">
                <div class="image-container">
                    <img id="autogui-image" src="" alt="Random Image">
                </div>
            </div>
            <div class="sidebar">
                <ul id="funcs"></ul>
            </div>
            <div class="buttons">
                <button class="nav-btn" onclick="showPreviousContent()">Previous</button>
                <button class="nav-btn" onclick="showNextContent()">Next</button>
                <button class="nav-btn" onclick="showRandomContent()">Random</button>
            </div>
        </div>

    </section>

    <section  class="col-md-8 col-md-offset-2 col-xs-12">
        <h3><span class="gradient-color">AutoGUI</span> Characteristics</h3>
        <p>Diversity of the verb-noun phrases of the AutoGUI dataset. The top 10 verbs and their top 5 following nouns are displayed to show that our dataset involve diverse UI functionalities.</p>

        <iframe src="verbnoun_sunburst.html" width="100%" height="600" style="border:none;"></iframe>

        <br><br>
        <p>The word cloud below illustrates the relative frequencies of the verbs that represent the primary intended actions described in the functionality annotations.</p>
        <p style="text-align:center;">
            <img src="img/word_cloud.png" width="75%" alt="AutoGUI word cloud">
        </p>
    </section>

    <section  class="col-md-8 col-md-offset-2 col-xs-12">
        <h3>Latest Works That Use AutoGUI</h3>
        <p><a href="https://arxiv.org/abs/2412.08073">UI-TARS</a>'s functionality descriptions come from our AutoGUI and its GUI state transition captioning may be also inspired by the funcitonality captioning pipeine of AutoGUI.
        </p>
    </section>



    <section  class="col-md-8 col-md-offset-2 col-xs-12">
        <h3>Disclaimer</h3>
        <p>This dataset has been compiled and disseminated exclusively for research purposes, to build universal GUI agents through the application of foundation models. Any commercial use of the data is not permitted.
        </p>
        <br>
        <h3>Contact</h3>
        <p>For any inquiries, please direct your questions to Hongxin Li and Yuntao Chen. Alternatively, issues can be submitted through the <a href="https://github.com/ZJULiHongxin/AutoGUI">AutoGUI</a> GitHub repository.</p>

        <h3>Acknowledgement</h3>
        <p>Our project codes are based on the <a href="https://github.com/QwenLM/Qwen-VL">Qwen-VL</a>, <a href="https://github.com/njucckevin/SeeClick">SeeClick</a>, and <a href="https://github.com/EvolvingLMMs-Lab/lmms-eval">lmms-eval</a>. We thank the authors for their open-source works.</p>
        <br><br>
    </section>
    
                
            </div>
        </div>
        
    </div>

</body>
</html>
